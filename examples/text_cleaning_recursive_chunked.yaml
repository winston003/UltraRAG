# 递归切割分块+LLM清洗+合并Pipeline配置
# 实现完整的文本清洗工作流：递归分块 -> LLM清洗 -> 合并

servers:
  custom: servers/custom

variables:
  input_file: "data/test_large_text.txt"
  output_file: "data/test_large_cleaned_recursive.txt"
  chunk_size: 2000
  chunk_overlap: 200
  max_depth: 3  # 递归切割最大深度
  min_chunk_size: 500  # 最小块大小，小于此值不再切割
  
pipeline:
  # 步骤1：加载原始文本
  - custom.load_text_file:
      input:
        file_path: $input_file
      output:
        text_content: original_text
      
  # 步骤2：递归切割分块
  - custom.chunk_text:
      input:
        text_content: original_text
        chunk_size: $chunk_size
        chunk_overlap: $chunk_overlap
      output:
        text_chunks: text_chunks
      
  # 步骤3：LLM清洗文本块
  - custom.clean_text_chunks:
      input:
        chunks: text_chunks
      output:
        cleaned_chunks: cleaned_chunks
        
  # 步骤4：合并清洗后的文本块
  - custom.merge_chunks:
      input:
        chunks: cleaned_chunks
      output:
        final_text: merged_text
        
  # 步骤5：保存最终文本
  - custom.save_text_file:
      input:
        text: merged_text
        file_path: $output_file
      output:
        save_result: save_status