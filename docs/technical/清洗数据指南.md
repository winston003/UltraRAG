基于UltraRAG框架，我可以为你设计一个高效的大规模文本清洗方案。对于3万汉字的录音转文本文件，建议采用分块+LLM清洗的策略。

## 清洗策略指南

### 1. 分块处理策略

- **块大小建议**：1000-2000字/块，避免LLM上下文限制
- **重叠处理**：块间重叠100-200字，保证语义连续性
- **清洗重点**：
  - 语音识别错误修正
  - 口语化表达规范化
  - 重复内容去除
  - 标点符号修正
  - 逻辑结构整理

### 2. LLM清洗提示词设计

```jinja
你是一个专业的文本清洗专家，请对以下录音转文本内容进行清洗：

清洗要求：
1. 修正语音识别错误（同音字、断句错误等）
2. 将口语化表达转为书面语
3. 去除重复内容和无意义的填充词
4. 修正标点符号和段落结构
5. 保持原意不变，提升可读性

原始文本：
{{text}}

请输出清洗后的文本：
```

## 样例代码实现

### 1. 创建自定义清洗服务

在 `servers/custom/src/cleaner.py` 中添加：

```python
import json
import os
from typing import Dict, List
from ultrarag.server import UltraRAG_MCP_Server

app = UltraRAG_MCP_Server("cleaner")

@app.tool(output="chunks->cleaned_chunks")
async def clean_text_chunks(chunks: List[str]) -> Dict[str, List[str]]:
    """清洗文本块"""
    cleaned_chunks = []
    for chunk in chunks:
        # 这里可以添加预处理逻辑
        cleaned_chunk = chunk.strip()
        cleaned_chunks.append(cleaned_chunk)
    return {"cleaned_chunks": cleaned_chunks}

@app.tool(output="cleaned_chunks->merged_text")
async def merge_chunks(cleaned_chunks: List[str]) -> Dict[str, str]:
    """合并清洗后的文本块"""
    merged_text = "\n\n".join(cleaned_chunks)
    return {"merged_text": merged_text}

@app.tool(output="merged_text,output_path->status")
async def save_cleaned_text(merged_text: str, output_path: str) -> Dict[str, str]:
    """保存清洗后的文本"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(merged_text)
    return {"status": f"Saved to {output_path}"}
```

### 2. 创建文本清洗Pipeline配置

创建 `examples/text_cleaning.yaml`：

```yaml
# 文本清洗Pipeline
servers:
  corpus: servers/corpus
  prompt: servers/prompt
  generation: servers/generation
  cleaner: servers/custom
  evaluation: servers/evaluation

pipeline:
- corpus.parse_documents          # 解析原始文本文件
- corpus.chunk_documents          # 分块处理
- prompt.qa_boxed                 # 使用清洗提示词
- generation.generate             # LLM清洗
- cleaner.clean_text_chunks       # 后处理清洗
- cleaner.merge_chunks            # 合并结果
- cleaner.save_cleaned_text       # 保存结果
- evaluation.evaluate             # 质量评估
```

### 3. 批量处理脚本

创建 `script/batch_clean.py`：

```python
import os
import glob
from pathlib import Path
import subprocess

def batch_clean_text_files(input_dir, output_dir, chunk_size=1500):
    """批量清洗文本文件"""
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
  
    # 支持多种文本格式
    text_files = glob.glob(str(input_dir / "*.txt")) + \
                 glob.glob(str(input_dir / "*.md")) + \
                 glob.glob(str(input_dir / "*.json"))
  
    for file_path in text_files:
        file_name = Path(file_path).stem
        output_file = output_dir / f"{file_name}_cleaned.txt"
      
        print(f"Processing: {file_path}")
      
        # 构建清洗命令
        cmd = [
            "ultrarag", "run", "examples/text_cleaning.yaml",
            "--input", str(file_path),
            "--output", str(output_file),
            "--chunk_size", str(chunk_size)
        ]
      
        try:
            subprocess.run(cmd, check=True)
            print(f"Completed: {output_file}")
        except subprocess.CalledProcessError as e:
            print(f"Error processing {file_path}: {e}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_dir", required=True, help="输入目录")
    parser.add_argument("--output_dir", required=True, help="输出目录")
    parser.add_argument("--chunk_size", type=int, default=1500, help="块大小")
    args = parser.parse_args()
  
    batch_clean_text_files(args.input_dir, args.output_dir, args.chunk_size)
```

## 使用方法

### 1. 安装依赖

```bash
pip install -e ."[corpus,generation]"
```

### 2. 配置参数

修改 `servers/corpus/parameter.yaml`：

```yaml
chunk_strategy: "sentence"  # 按句子分块
chunk_size: 1500            # 每块1500字
tokenizer_name_or_path: "gpt2"  # 使用gpt2分词器
```

### 3. 运行批量清洗

```bash
python script/batch_clean.py \
    --input_dir /path/to/raw_texts \
    --output_dir /path/to/cleaned_texts \
    --chunk_size 1500
```

## 性能优化建议

1. **并行处理**：修改generation server的并发数
2. **GPU加速**：使用vLLM部署本地LLM
3. **缓存机制**：对已清洗的块进行缓存
4. **增量处理**：支持断点续传

这个方案充分利用了UltraRAG的组件化架构，可以高效处理大规模文本清洗任务。
