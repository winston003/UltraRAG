要为 UltraRAG 实现稀疏检索和完整的元数据检索能力，需要从以下几个方面进行扩展：

## 1. 稀疏检索实现方案

### 1.1 集成 BM25 或 SPLADE

在 [`retriever.py`](servers/retriever/src/retriever.py) 中添加稀疏检索支持：

```python
# 添加必要的导入
try:
    from rank_bm25 import BM25Okapi
    import jieba  # 中文分词
    from transformers import AutoTokenizer, AutoModel
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False

class Retriever:
    def __init__(self, mcp_inst: UltraRAG_MCP_Server):
        # ... 现有代码 ...
      
        # 添加稀疏检索工具
        mcp_inst.tool(
            self.retriever_init_sparse,
            output="corpus_path,tokenize_method,language->None",
        )
        mcp_inst.tool(
            self.retriever_index_sparse,
            output="corpus_path,sparse_index_path,overwrite->None",
        )
        mcp_inst.tool(
            self.retriever_search_sparse,
            output="q_ls,top_k->ret_psg",
        )
        mcp_inst.tool(
            self.retriever_search_hybrid,
            output="q_ls,top_k,dense_weight,sparse_weight,filter_expr->ret_psg",
        )

    def retriever_init_sparse(
        self,
        corpus_path: str,
        tokenize_method: str = "jieba",  # jieba, spacy, whitespace
        language: str = "zh",  # zh, en
    ):
        """初始化稀疏检索器"""
        if not BM25_AVAILABLE:
            raise ImportError("请安装 rank_bm25 和 jieba: pip install rank_bm25 jieba")
          
        self.tokenize_method = tokenize_method
        self.language = language
      
        # 加载语料库
        self.contents = []
        with jsonlines.open(corpus_path, mode="r") as reader:
            self.contents = [item["contents"] for item in reader]
      
        # 分词处理
        self.tokenized_corpus = self._tokenize_corpus(self.contents)
      
        # 初始化BM25
        self.bm25 = BM25Okapi(self.tokenized_corpus)
      
        app.logger.info("稀疏检索器初始化完成")

    def _tokenize_corpus(self, corpus):
        """语料库分词"""
        tokenized = []
        for text in corpus:
            if self.tokenize_method == "jieba" and self.language == "zh":
                tokens = list(jieba.cut(text))
            elif self.tokenize_method == "whitespace":
                tokens = text.split()
            else:
                tokens = text.lower().split()
            tokenized.append(tokens)
        return tokenized

    def retriever_index_sparse(
        self,
        corpus_path: str,
        sparse_index_path: Optional[str] = None,
        overwrite: bool = False,
    ):
        """构建稀疏索引"""
        if sparse_index_path is None:
            current_file = os.path.abspath(__file__)
            project_root = os.path.dirname(os.path.dirname(current_file))
            sparse_index_path = os.path.join(project_root, "output", "sparse_index.pkl")
      
        if not overwrite and os.path.exists(sparse_index_path):
            app.logger.info("稀疏索引已存在，跳过构建")
            return
          
        import pickle
      
        # 保存BM25索引和分词结果
        sparse_data = {
            'bm25': self.bm25,
            'tokenized_corpus': self.tokenized_corpus,
            'contents': self.contents
        }
      
        os.makedirs(os.path.dirname(sparse_index_path), exist_ok=True)
        with open(sparse_index_path, 'wb') as f:
            pickle.dump(sparse_data, f)
      
        app.logger.info("稀疏索引构建完成")

    async def retriever_search_sparse(
        self,
        query_list: List[str],
        top_k: int = 5,
    ) -> Dict[str, List[List[str]]]:
        """稀疏检索搜索"""
        if isinstance(query_list, str):
            query_list = [query_list]
      
        rets = []
        for query in query_list:
            # 查询分词
            tokenized_query = self._tokenize_corpus([query])[0]
          
            # BM25检索
            bm25_scores = self.bm25.get_scores(tokenized_query)
          
            # 获取top_k结果
            top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
          
            cur_ret = [self.contents[idx] for idx in top_indices if bm25_scores[idx] > 0]
            rets.append(cur_ret)
      
        return {"ret_psg": rets}
```

### 1.2 混合检索实现

```python
    async def retriever_search_hybrid(
        self,
        query_list: List[str],
        top_k: int = 5,
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3,
        lancedb_path: str = "",
        table_name: str = "",
        filter_expr: Optional[str] = None,
    ) -> Dict[str, List[List[str]]]:
        """混合检索：稠密 + 稀疏"""
        if isinstance(query_list, str):
            query_list = [query_list]
      
        rets = []
      
        for query in query_list:
            # 1. 稠密检索
            dense_results = await self._dense_search(query, top_k * 2, lancedb_path, table_name, filter_expr)
          
            # 2. 稀疏检索  
            sparse_results = await self._sparse_search(query, top_k * 2)
          
            # 3. 结果融合
            fused_results = self._fusion_results(dense_results, sparse_results, dense_weight, sparse_weight, top_k)
          
            rets.append(fused_results)
      
        return {"ret_psg": rets}

    async def _dense_search(self, query, top_k, lancedb_path, table_name, filter_expr):
        """稠密检索内部方法"""
        # 复用现有的LanceDB检索逻辑
        return await self.retriever_search_lancedb(
            [query], top_k, "", False, lancedb_path, table_name, filter_expr
        )

    async def _sparse_search(self, query, top_k):
        """稀疏检索内部方法"""
        return await self.retriever_search_sparse([query], top_k)

    def _fusion_results(self, dense_results, sparse_results, dense_weight, sparse_weight, top_k):
        """结果融合"""
        dense_scores = {}
        sparse_scores = {}
      
        # 计算稠密检索分数
        for i, doc in enumerate(dense_results["ret_psg"][0]):
            dense_scores[doc] = (top_k * 2 - i) / (top_k * 2)  # 归一化分数
      
        # 计算稀疏检索分数
        for i, doc in enumerate(sparse_results["ret_psg"][0]):
            sparse_scores[doc] = (top_k * 2 - i) / (top_k * 2)  # 归一化分数
      
        # 融合分数
        fused_scores = {}
        all_docs = set(dense_scores.keys()) | set(sparse_scores.keys())
      
        for doc in all_docs:
            dense_score = dense_scores.get(doc, 0)
            sparse_score = sparse_scores.get(doc, 0)
            fused_scores[doc] = dense_weight * dense_score + sparse_weight * sparse_score
      
        # 按融合分数排序
        sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
      
        return [doc for doc, score in sorted_docs[:top_k]]
```

## 2. 元数据检索增强方案

### 2.1 修改数据结构

```python
    def retriever_index_lancedb_enhanced(
        self,
        embedding_path: str,
        lancedb_path: str,
        table_name: str,
        metadata_path: Optional[str] = None,  # 新增：元数据文件路径
        overwrite: bool = False,
    ):
        """增强版LanceDB索引，支持元数据"""
        try:
            import lancedb
            import pyarrow as pa
        except ImportError:
            err_msg = "lancedb 和 pyarrow 未安装。请运行: pip install lancedb pyarrow"
            raise ImportError(err_msg)

        # 加载向量数据
        embedding = np.load(embedding_path)
      
        # 加载元数据
        metadata = []
        if metadata_path and os.path.exists(metadata_path):
            with jsonlines.open(metadata_path, mode="r") as reader:
                metadata = list(reader)
        else:
            # 如果没有元数据文件，创建基本元数据
            metadata = [{"id": str(i), "content": self.contents[i]} for i in range(len(self.contents))]
      
        # 创建包含元数据的DataFrame
        data = []
        for i, (meta, vec) in enumerate(zip(metadata, embedding)):
            record = {
                "id": str(i),
                "vector": vec,
                "content": meta.get("content", self.contents[i]),
                "title": meta.get("title", ""),
                "author": meta.get("author", ""),
                "date": meta.get("date", ""),
                "category": meta.get("category", ""),
                "tags": meta.get("tags", []),
                "language": meta.get("language", "zh"),
                "length": len(self.contents[i]),
                "source": meta.get("source", "unknown")
            }
            data.append(record)
      
        df = pd.DataFrame(data)
      
        # 连接LanceDB并创建表
        db = lancedb.connect(lancedb_path)
      
        if table_name in db.table_names() and not overwrite:
            app.logger.info(f"LanceDB表 '{table_name}' 已存在，跳过构建")
            return
      
        # 创建表，指定数据类型
        schema = pa.schema([
            pa.field("id", pa.string()),
            pa.field("vector", pa.list_(pa.float32())),
            pa.field("content", pa.string()),
            pa.field("title", pa.string()),
            pa.field("author", pa.string()),
            pa.field("date", pa.string()),
            pa.field("category", pa.string()),
            pa.field("tags", pa.list_(pa.string())),
            pa.field("language", pa.string()),
            pa.field("length", pa.int32()),
            pa.field("source", pa.string())
        ])
      
        db.create_table(table_name, data=df, schema=schema)
        app.logger.info("增强版LanceDB索引构建完成")
```

### 2.2 高级元数据检索

```python
    async def retriever_search_with_metadata(
        self,
        query_list: List[str],
        top_k: int = 5,
        lancedb_path: str = "",
        table_name: str = "",
        metadata_filters: Optional[Dict[str, Any]] = None,
        date_range: Optional[Dict[str, str]] = None,
        categories: Optional[List[str]] = None,
        tags: Optional[List[str]] = None,
        language: Optional[str] = None,
        min_length: Optional[int] = None,
        max_length: Optional[int] = None,
        sources: Optional[List[str]] = None,
    ) -> Dict[str, List[List[str]]]:
        """支持复杂元数据过滤的检索"""
        try:
            import lancedb
        except ImportError:
            raise ImportError("lancedb 未安装")

        if isinstance(query_list, str):
            query_list = [query_list]
      
        # 构建过滤条件
        filter_conditions = []
      
        # 日期范围过滤
        if date_range:
            start_date = date_range.get("start")
            end_date = date_range.get("end")
            if start_date and end_date:
                filter_conditions.append(f"date >= '{start_date}' AND date <= '{end_date}'")
            elif start_date:
                filter_conditions.append(f"date >= '{start_date}'")
            elif end_date:
                filter_conditions.append(f"date <= '{end_date}'")
      
        # 分类过滤
        if categories:
            categories_str = "', '".join(categories)
            filter_conditions.append(f"category IN ('{categories_str}')")
      
        # 标签过滤
        if tags:
            for tag in tags:
                filter_conditions.append(f"'{tag}' IN tags")
      
        # 语言过滤
        if language:
            filter_conditions.append(f"language = '{language}'")
      
        # 长度过滤
        if min_length is not None:
            filter_conditions.append(f"length >= {min_length}")
        if max_length is not None:
            filter_conditions.append(f"length <= {max_length}")
      
        # 来源过滤
        if sources:
            sources_str = "', '".join(sources)
            filter_conditions.append(f"source IN ('{sources_str}')")
      
        # 自定义元数据过滤
        if metadata_filters:
            for key, value in metadata_filters.items():
                if isinstance(value, list):
                    values_str = "', '".join([str(v) for v in value])
                    filter_conditions.append(f"{key} IN ('{values_str}')")
                elif isinstance(value, str):
                    filter_conditions.append(f"{key} = '{value}'")
                else:
                    filter_conditions.append(f"{key} = {value}")
      
        # 组合过滤条件
        filter_expr = " AND ".join(filter_conditions) if filter_conditions else None
      
        # 执行检索
        return await self.retriever_search_lancedb(
            query_list, top_k, "", False, lancedb_path, table_name, filter_expr
        )
```

## 3. 配置文件更新

在 `examples/` 目录下添加新的配置文件示例：

```yaml
# examples/hybrid_retrieval.yaml
pipeline:
  - retriever.retriever_init_sparse
  - retriever.retriever_init
  - retriever.retriever_embed
  - retriever.retriever_index_lancedb_enhanced
  - retriever.retriever_index_sparse
  - retriever.retriever_search_hybrid

servers:
  retriever:
    path: servers/retriever
```

## 4. 安装依赖

在 `pyproject.toml` 中添加新依赖：

```toml
[tool.poetry.dependencies]
rank_bm25 = "^0.2.2"
jieba = "^0.42.1"
pyarrow = "^14.0.0"
```

## 实施步骤

1. **安装依赖**：添加稀疏检索和元数据处理库
2. **修改检索器**：扩展 [`Retriever`](servers/retriever/src/retriever.py) 类
3. **更新配置**：添加混合检索配置文件
4. **测试验证**：确保新功能正常工作
5. **文档更新**：在 [配置检索器和搜索](11-configuring-retrievers-and-search) 中添加说明

这样就能实现完整的稀疏检索和高级元数据检索功能，大幅提升 UltraRAG 的检索能力。

[配置检索器和搜索](11-configuring-retrievers-and-search)
[构建自定义组件](16-building-custom-components)
[高级检索策略](8-advanced-retrieval-strategies-ircot-iterretgen-rankcot)
